#%%
"""
04_time_to_AKI_ehrapy.py
=========================
Fragestellung: "Wie effektiv unterstützt das Framework ehrapy die Identifikation von Risikofaktoren für AKI
bei Kindern nach Herzoperationen anhand eines angereicherten Routinedatensatzes?"

Ansatz (ehrapy-first):
- **ehrapy/AnnData** ist die **zentrale Datenstruktur**. Alle abgeleiteten Variablen und Metadaten
  werden in `adata.obs` bzw. `adata.uns` abgelegt (Provenance!).
- Visualisierungen/Modelle verwenden stabile Ökosystem-Pakete (`lifelines`, `matplotlib`, `statsmodels/sklearn`).
- Ergebnis: Reproduzierbare Pipeline, die methodisch in ehrapy eingebettet bleibt.

Dieser Schritt (S1): Zeit-zu-Ereignis (Kaplan–Meier) für AKI ≤ 7 Tage
- Definiert **Index-OP** je AKI (letzte OP vor AKI im 0–7-Tage-Fenster), um Doppelzählungen zu vermeiden.
- Erzeugt `obs`-Variablen: `time_0_7`, `event_idx`.
- Schreibt Parameter/Provenance nach `adata.uns['survival_0_7']`.
- Exportiert CSV + 2 Plots; speichert Version der AnnData-Datei (H5AD) als S1.

Voraussetzungen
---------------
- Conda-Env: ehrapy_env (Python ≥ 3.10)
- Pakete: ehrapy, anndata, pandas, numpy, lifelines, matplotlib

Installation (falls nötig):
  conda install -c conda-forge ehrapy anndata pandas numpy lifelines matplotlib

"""
from __future__ import annotations

import os
from dataclasses import dataclass
from typing import Dict, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ehrapy/AnnData laden (robust mit Fallback)
try:
    import ehrapy as ep  # type: ignore
except Exception:
    ep = None  # Fallback erlaubt, Kern bleibt AnnData

from anndata import read_h5ad, AnnData

try:
    from lifelines import KaplanMeierFitter
except Exception as e:
    raise SystemExit(
        "lifelines fehlt. Bitte installieren:\n"
        "  conda install -c conda-forge lifelines\n\n"
        f"Technischer Hinweis: {e}"
    )


# ---------------------------------
# 1) KONFIGURATION
# ---------------------------------
@dataclass
class Config:
    PATH_H5AD: str = (
        "/Users/fa/Library/Mobile Documents/com~apple~CloudDocs/cs-transfer/aki_ops_master.h5ad"
    )
    SAVE_DIR: str = (
        "/Users/fa/Library/Mobile Documents/com~apple~CloudDocs/cs-transfer/Diagramme"
    )
    OUT_H5AD: str = (
        "/Users/fa/Library/Mobile Documents/com~apple~CloudDocs/cs-transfer/aki_ops_master_S1_survival.h5ad"
    )
    WINDOW_DAYS: int = 7

CFG = Config()


# ---------------------------------
# 2) HILFSFUNKTIONEN (ehrapy-first)
# ---------------------------------

def load_adata() -> AnnData:
    """Lädt die AnnData-Struktur. Bevorzugt `ep.io.read_h5ad`, sonst Fallback `anndata.read_h5ad`.

    *Warum so?* Damit bleibt die Pipeline **ehrapy-first**, ist aber robust, falls einzelne I/O-Funktionen
    in Ihrer ehrapy-Version fehlen.
    """
    if ep is not None and hasattr(ep, "io") and hasattr(ep.io, "read_h5ad"):
        adata = ep.io.read_h5ad(CFG.PATH_H5AD)  # type: ignore[attr-defined]
    else:
        adata = read_h5ad(CFG.PATH_H5AD)
    return adata


def _ensure_datetime(df: pd.DataFrame, cols: Tuple[str, ...]) -> pd.DataFrame:
    """Robuste Datums-Konvertierung mit `errors='coerce'` (fehlerhafte Einträge -> NaT)."""
    df = df.copy()
    for c in cols:
        if c in df.columns:
            df[c] = pd.to_datetime(df[c], errors="coerce")
    return df


def annotate_provenance(adata: AnnData, key: str, meta: Dict) -> None:
    """Speichert Parameter/Provenance in `adata.uns[key]`.

    Beispiele: Fenstergröße, Versionshinweise, Anzahl Ereignisse.
    """
    if adata.uns is None:
        adata.uns = {}
    adata.uns[key] = meta


def define_event_index_on_adata(adata: AnnData) -> None:
    """Definiert den **Index-OP** je AKI-Episode und erzeugt `event_idx` in `obs`.

    Logik:
    - Kandidaten = OPs mit `AKI_linked_0_7 == 1` und gültigem `AKI_Start`.
    - Auswahl = letzte OP mit `Surgery_End` ≤ `AKI_Start` innerhalb der Gruppe (`PMID`, `AKI_Start`).
    - Ergebnis: `obs['event_idx']` ∈ {0,1}.
    """
    df = adata.obs.copy()

    # duration_hours ggf. aus X holen (falls nur 1 Var in var vorhanden)
    if "duration_hours" not in df.columns and "duration_hours" in adata.var_names:
        df = pd.concat([df, adata.to_df()[["duration_hours"]]], axis=1)

    # Typen/Zeiten vereinheitlichen
    df = _ensure_datetime(df, ("Surgery_Start", "Surgery_End", "AKI_Start"))
    if "days_to_AKI" in df.columns:
        df["days_to_AKI"] = pd.to_numeric(df["days_to_AKI"], errors="coerce")

    # Binärfelder harmonisieren
    for b in ("AKI_linked", "AKI_linked_0_7"):
        if b in df.columns:
            df[b] = (
                df[b].astype(str).str.strip().replace({"True": 1, "False": 0, "nan": np.nan, "None": np.nan})
            )
            df[b] = pd.to_numeric(df[b], errors="coerce").fillna(0).astype(int)

    # Offensichtliche Datenfehler entfernen (negatives Intervall nicht zulassen)
    if "days_to_AKI" in df.columns:
        df = df[df["days_to_AKI"].isna() | (df["days_to_AKI"] >= 0)].copy()

    # Ereignis-Index initialisieren
    df["event_idx"] = 0

    if "AKI_linked_0_7" in df.columns and df["AKI_linked_0_7"].sum() > 0:
        cand = df[(df["AKI_linked_0_7"] == 1) & df["AKI_Start"].notna()].copy()
        cand = cand[cand["Surgery_End"].notna()]
        cand = cand[cand["Surgery_End"] <= cand["AKI_Start"]]

        if not cand.empty:
            idx_rows = (
                cand.sort_values(["PMID", "AKI_Start", "Surgery_End"]).groupby(["PMID", "AKI_Start"], observed=True, as_index=False).tail(1).index
            )
            df.loc[idx_rows, "event_idx"] = 1

            multi_counts = cand.groupby(["PMID", "AKI_Start"], observed=True).size().rename("n_ops_in_window").reset_index()
            n_multi = int((multi_counts["n_ops_in_window"] > 1).sum())
        else:
            n_multi = 0
    else:
        n_multi = 0

    # Ergebnisse zurück nach adata.obs (Index beibehalten)
    adata.obs.loc[df.index, "event_idx"] = df["event_idx"].astype(int)

    # Provenance
    annotate_provenance(
        adata,
        key="survival_0_7_index",
        meta={
            "window_days": CFG.WINDOW_DAYS,
            "note": "Index-OP = letzte OP vor AKI im 0–7-Tage-Fenster",
            "n_events_marked": int(df["event_idx"].sum()),
            "n_total": int(df.shape[0]),
            "n_multi_ops_per_aki": int(n_multi),
        },
    )


def build_survival_on_adata(adata: AnnData) -> pd.DataFrame:
    """Erzeugt `time_0_7` (Tage) und nutzt `event_idx` aus `obs`.

    - Für Index-OPs: `time_0_7 = min(days_to_AKI, 7)`.
    - Für andere gelinkte OPs: `time_0_7 = min(days_to_AKI, 7)`, aber `event_idx = 0`.
    - Für unverbundene OPs: `time_0_7 = 7`, `event_idx = 0`.

    Speichert `time_0_7` dauerhaft in `adata.obs` und gibt den kompakten **Survival-DataFrame** zurück.
    """
    df = adata.obs.copy()

    if "days_to_AKI" not in df.columns:
        raise KeyError("'days_to_AKI' fehlt in adata.obs – bitte beim Preprocessing erzeugen.")

    if "event_idx" not in df.columns:
        raise KeyError("'event_idx' fehlt. Bitte zuerst define_event_index_on_adata() ausführen.")

    # Basis: zensiert bei WINDOW_DAYS
    df["time_0_7"] = float(CFG.WINDOW_DAYS)

    mask_evt = df["event_idx"] == 1
    df.loc[mask_evt, "time_0_7"] = np.clip(df.loc[mask_evt, "days_to_AKI"], 0, CFG.WINDOW_DAYS)

    mask_non_index_linked = (df.get("AKI_linked_0_7", 0) == 1) & (~mask_evt)
    df.loc[mask_non_index_linked, "time_0_7"] = np.clip(df.loc[mask_non_index_linked, "days_to_AKI"], 0, CFG.WINDOW_DAYS)

    # negative Zeiten vermeiden
    df.loc[df["time_0_7"] < 0, "time_0_7"] = 0.0

    # zurückschreiben
    adata.obs.loc[df.index, "time_0_7"] = df["time_0_7"].astype(float)

    # kompakten Survival-DF bilden
    keep = [c for c in ["time_0_7", "event_idx", "days_to_AKI", "AKI_linked_0_7", "Surgery_End", "AKI_Start", "PMID", "SMID", "Procedure_ID", "duration_hours", "Sex_norm"] if c in df.columns]
    surv = df[keep].copy()

    # Provenance
    annotate_provenance(
        adata,
        key="survival_0_7",
        meta={
            "window_days": CFG.WINDOW_DAYS,
            "n_rows": int(surv.shape[0]),
            "n_events": int((surv["event_idx"] == 1).sum()),
            "time_summary": {
                "min": float(surv["time_0_7"].min()),
                "median": float(surv["time_0_7"].median()),
                "max": float(surv["time_0_7"].max()),
            },
        },
    )

    return surv


def plot_km_and_cuminc(surv: pd.DataFrame, save_dir: str) -> Tuple[str, str]:
    """Speichert zwei Plots: KM (Überleben ohne AKI) und kumulative Inzidenz (1−S(t))."""
    os.makedirs(save_dir, exist_ok=True)

    # KM
    kmf = KaplanMeierFitter()
    T = surv["time_0_7"].astype(float)
    E = surv["event_idx"].astype(int)
    kmf.fit(T, event_observed=E, label="AKI ≤ 7 Tage")

    plt.figure(figsize=(7, 5))
    ax = kmf.plot(ci_show=True)
    ax.set_title("Kaplan–Meier: Zeit bis AKI ≤ 7 Tage ab OP-Ende")
    ax.set_xlabel("Tage seit OP-Ende")
    ax.set_ylabel("Überlebenswahrscheinlichkeit ohne AKI")
    ax.grid(True, alpha=0.3)
    km_path = os.path.join(save_dir, "KM_0_7_overall.png")
    plt.tight_layout(); plt.savefig(km_path, dpi=150); plt.close()

    # kumulative Inzidenz
    cuminc = 1.0 - kmf.survival_function_
    plt.figure(figsize=(7, 5))
    plt.step(cuminc.index.values, cuminc.values.flatten(), where="post")
    plt.title("Kumulative Inzidenz: AKI ≤ 7 Tage ab OP-Ende")
    plt.xlabel("Tage seit OP-Ende")
    plt.ylabel("Anteil mit AKI (1 − S(t))")
    plt.grid(True, alpha=0.3)
    ci_path = os.path.join(save_dir, "KM_0_7_cuminc.png")
    plt.tight_layout(); plt.savefig(ci_path, dpi=150); plt.close()

    return km_path, ci_path


# ---------------------------------
# 3) SUBGRUPPEN & BASISBESCHREIBUNG (S2)
# ---------------------------------

def compute_reop_and_duration_features(adata: AnnData) -> None:
    """Berechnet zusätzliche Variablen in `adata.obs`:
    - `is_reop` : 1 = Re-Operation (Patient hatte bereits eine frühere OP), sonst 0.
    - `duration_tertile` : kategorial (1/2/3) basierend auf `duration_hours`.

    Begründung:
    - Re-OPs sind klinisch relevant (höheres Risiko?).
    - Tertile der OP-Dauer erlauben eine robuste, nicht-parametrische Subgruppenbildung.
    """
    df = adata.obs.copy()

    # Sicherstellen, dass Zeitvariable für Sortierung existiert
    if "Surgery_End" in df.columns:
        df = df.copy()
        df["Surgery_End"] = pd.to_datetime(df["Surgery_End"], errors="coerce")
        # Reihenfolge je Patient
        df = df.sort_values(["PMID", "Surgery_End"]).copy()
        # Laufender Zähler pro Patient, beginnend bei 1
        df["op_seq"] = df.groupby("PMID", observed=True).cumcount() + 1
        df["is_reop"] = (df["op_seq"] > 1).astype(int)
        # zurückschreiben
        adata.obs.loc[df.index, "is_reop"] = df["is_reop"].astype(int)
    else:
        adata.obs["is_reop"] = 0  # konservativer Fallback

    # Dauer (in Stunden) prüfen
    if "duration_hours" not in adata.obs.columns and "duration_hours" in adata.var_names:
        # falls noch nicht in obs
        adata.obs = pd.concat([adata.obs, adata.to_df()[["duration_hours"]]], axis=1)

    if "duration_hours" in adata.obs.columns:
        # Tertile nur über valide Werte bilden
        dh = adata.obs["duration_hours"].astype(float)
        valid_mask = dh.notna()
        try:
            q = dh[valid_mask].quantile([1/3, 2/3]).values
            # 1: kurz, 2: mittel, 3: lang
            tertile = pd.Series(np.nan, index=adata.obs.index, dtype=float)
            tertile.loc[valid_mask & (dh <= q[0])] = 1
            tertile.loc[valid_mask & (dh > q[0]) & (dh <= q[1])] = 2
            tertile.loc[valid_mask & (dh > q[1])] = 3
            adata.obs["duration_tertile"] = tertile.astype("Int64")
        except Exception:
            adata.obs["duration_tertile"] = pd.Series(np.nan, index=adata.obs.index, dtype="Int64")
    else:
        adata.obs["duration_tertile"] = pd.Series(np.nan, index=adata.obs.index, dtype="Int64")


def km_by_group(surv: pd.DataFrame, group_col: str, save_dir: str, fname: str, order: list | None = None, label_map: dict | None = None) -> str:
    """Zeichnet KM-Kurven **stratifiziert** nach `group_col`.

    - `order` legt die Reihenfolge der Gruppen fest (optional).
    - `label_map` erlaubt schönere Legenden-Texte.
    """
    from lifelines import KaplanMeierFitter

    os.makedirs(save_dir, exist_ok=True)

    kmf = KaplanMeierFitter()

    plt.figure(figsize=(7, 5))

    groups = surv[group_col].dropna().unique().tolist()
    if order is not None:
        groups = [g for g in order if g in groups]

    for g in groups:
        sub = surv[surv[group_col] == g]
        if sub.empty:
            continue
        T = sub["time_0_7"].astype(float)
        E = sub["event_idx"].astype(int)
        label = label_map.get(g, str(g)) if label_map else str(g)
        kmf.fit(T, event_observed=E, label=label)
        kmf.plot(ci_show=False)

    plt.title(f"Kaplan–Meier (0–7 Tage) nach {group_col}")
    plt.xlabel("Tage seit OP-Ende")
    plt.ylabel("Überlebenswahrscheinlichkeit ohne AKI")
    plt.grid(True, alpha=0.3)
    plt.legend(title=group_col)

    out_path = os.path.join(save_dir, fname)
    plt.tight_layout(); plt.savefig(out_path, dpi=150); plt.close()

    return out_path


def make_table1_op_level(surv: pd.DataFrame, save_dir: str) -> str:
    """Erstellt eine einfache **Table 1** auf OP-Level, stratifiziert nach `event_idx` (0/1).

    Hinweis: `event_idx==1` bezeichnet **Index-OPs mit AKI**; `0` sind alle übrigen OPs (inkl. zensiert).
    Für die Bachelorarbeit ist das als *OP-basierte* Baseline sinnvoll, die *Patienten-basierte* Variante
    (erste OP pro Patient) können wir als Sensitivitätsanalyse ergänzen.
    """
    os.makedirs(save_dir, exist_ok=True)

    def _summary_num(x: pd.Series) -> dict:
        x = pd.to_numeric(x, errors="coerce")
        return {
            "n": int(x.notna().sum()),
            "mean": float(x.mean()),
            "sd": float(x.std()),
            "median": float(x.median()),
            "q1": float(x.quantile(0.25)),
            "q3": float(x.quantile(0.75)),
        }

    rows = []

    for grp, gdf in surv.groupby("event_idx", observed=True):
        label = "AKI-Index-OP" if grp == 1 else "Keine AKI-Index-OP"

        # Dauer
        if "duration_hours" in gdf.columns:
            rows.append({"Gruppe": label, "Variable": "duration_hours", **_summary_num(gdf["duration_hours"])})

        # Kategoriale: Geschlecht
        if "Sex_norm" in gdf.columns:
            vc = gdf["Sex_norm"].value_counts(dropna=False)
            n = int(len(gdf))
            for k, v in vc.items():
                rows.append({"Gruppe": label, "Variable": f"Sex_norm={k}", "n": int(v), "pct": float(100.0*v/n)})

        # Kategoriale: Re-OP
        if "is_reop" in gdf.columns:
            vc = gdf["is_reop"].value_counts(dropna=False)
            n = int(len(gdf))
            for k, v in vc.items():
                rows.append({"Gruppe": label, "Variable": f"is_reop={k}", "n": int(v), "pct": float(100.0*v/n)})

        # Kategoriale: Dauer-Tertile
        if "duration_tertile" in gdf.columns:
            vc = gdf["duration_tertile"].value_counts(dropna=False)
            n = int(len(gdf))
            for k, v in vc.items():
                rows.append({"Gruppe": label, "Variable": f"duration_tertile={k}", "n": int(v), "pct": float(100.0*v/n)})

    out = pd.DataFrame(rows)
    csv_path = os.path.join(save_dir, "table1_op_level.csv")
    out.to_csv(csv_path, index=False)
    return csv_path


# ---------------------------------
# 3) HAUPTABLAUF (S1)
# ---------------------------------

def main():
    # A) Laden (ehrapy-first, Fallback vorhanden)
    adata = load_adata()

    # B) Index-OPs definieren
    define_event_index_on_adata(adata)

    # C) Survival-Variablen aufbauen und zurück in adata.obs schreiben
    surv = build_survival_on_adata(adata)

    # D) CSV-Export (für externe Re-Use und QA)
    os.makedirs(CFG.SAVE_DIR, exist_ok=True)
    csv_path = os.path.join(CFG.SAVE_DIR, "survival_dataset_0_7.csv")
    surv.to_csv(csv_path, index=False)
    print(f"Survival-CSV gespeichert: {csv_path}")

    # E) Plots
    km_path, ci_path = plot_km_and_cuminc(surv, CFG.SAVE_DIR)
    print(f"Plots gespeichert: {km_path} | {ci_path}")

    # F) Versionierte H5AD-Datei als S1 persistieren (ehrapy/AnnData)
    adata.write_h5ad(CFG.OUT_H5AD)
    print(f"AnnData (S1) gespeichert: {CFG.OUT_H5AD}")

    print("\nFERTIG (S1): ehrapy-first Survival-Setup abgeschlossen.\n")


def main_s2():
    """S2: Subgruppenplots + Table 1 (ehrapy-integriert).

    Lädt, falls vorhanden, die S1-Datei (`CFG.OUT_H5AD`), ergänzt Subgruppen-Features und erzeugt
    stratifizierte KM-Kurven sowie eine einfache Table 1.
    """
    # Laden: bevorzugt die versionierte S1-Datei
    adata = read_h5ad(CFG.OUT_H5AD) if os.path.exists(CFG.OUT_H5AD) else load_adata()

    # Subgruppen-Features berechnen
    compute_reop_and_duration_features(adata)

    # Survival-DF aus obs zusammenstellen
    needed = ["time_0_7", "event_idx", "duration_hours", "Sex_norm", "is_reop", "duration_tertile"]
    surv_cols = [c for c in needed if c in adata.obs.columns]
    surv = adata.obs[surv_cols].copy()

    # KM nach Geschlecht
    if "Sex_norm" in surv.columns:
        _ = km_by_group(
            surv=surv,
            group_col="Sex_norm",
            save_dir=CFG.SAVE_DIR,
            fname="KM_0_7_by_sex.png",
            order=None,
            label_map=None,
        )

    # KM Erst-OP vs Re-OP
    if "is_reop" in surv.columns:
        _ = km_by_group(
            surv=surv,
            group_col="is_reop",
            save_dir=CFG.SAVE_DIR,
            fname="KM_0_7_by_reop.png",
            order=[0, 1],
            label_map={0: "Erst-OP", 1: "Re-OP"},
        )

    # KM nach Dauer-Tertilen
    if "duration_tertile" in surv.columns:
        _ = km_by_group(
            surv=surv,
            group_col="duration_tertile",
            save_dir=CFG.SAVE_DIR,
            fname="KM_0_7_by_duration_tertile.png",
            order=[1, 2, 3],
            label_map={1: "kurz", 2: "mittel", 3: "lang"},
        )

    # Table 1
    t1_path = make_table1_op_level(surv, CFG.SAVE_DIR)
    print(f"Table 1 gespeichert: {t1_path}")

    # Speichern (gleicher OUT-Pfad, aktualisierte obs)
    adata.write_h5ad(CFG.OUT_H5AD)
    print(f"AnnData (S1+S2) gespeichert: {CFG.OUT_H5AD}")





#%%
# ---------------------------------
# S4) PRÄDIKTION & INFERENZ – Logistische Regression (ehrapy-first)
#     - GLM (Binomial, Logit) mit **cluster-robusten SE** (Cluster = PMID)
#     - Cross-Validation mit **GroupKFold (PMID)** und scikit-learn-Pipeline
#     - Outputs: OR-Tabelle (CSV), ROC/PR/Calibration-Plots + Metriken
#     - Ergebnisse in adata.uns['S4_glm'] und adata.uns['S4_cv'] (nur einfache Typen)
# ---------------------------------

# ============================
# S4 – Logit + GroupKFold CV
# ============================
import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt

def _prepare_model_df(adata):
    """Baut den Modell-DataFrame (OP-basiert)."""
    need = ["PMID","event_idx","duration_hours","is_reop","Sex_norm"]
    for c in need:
        if c not in adata.obs.columns:
            adata.obs[c] = np.nan
    df = adata.obs[need].copy()

    # strikte Typen
    df["event_idx"] = pd.to_numeric(df["event_idx"], errors="coerce").astype("Int64")
    df = df[df["event_idx"].notna() & df["PMID"].notna()].copy()
    df["event_idx"] = df["event_idx"].astype(int)
    df["duration_hours"] = pd.to_numeric(df["duration_hours"], errors="coerce")
    df["is_reop"] = pd.to_numeric(df["is_reop"], errors="coerce").fillna(0).astype(int)
    df["Sex_norm"] = df["Sex_norm"].astype(str).replace({"nan": np.nan, "None": np.nan}).fillna("Missing")
    return df

def _fit_glm_clustered(df, save_dir):
    """GLM (Binomial Logit) + cluster-robuste SE (Cluster=PMID). Speichert OR-Tabelle."""
    # imports lokal, damit Skript auch ohne S4-Pakete lauffähig bleibt
    import statsmodels.api as sm

    y = pd.to_numeric(df["event_idx"], errors="coerce").astype(int).values

    X = df[["duration_hours","is_reop","Sex_norm"]].copy()
    X["duration_hours"] = pd.to_numeric(X["duration_hours"], errors="coerce").fillna(X["duration_hours"].median())
    X["is_reop"] = pd.to_numeric(X["is_reop"], errors="coerce").fillna(0).astype(int)
    X["Sex_norm"] = X["Sex_norm"].astype(str).replace({"nan": np.nan, "None": np.nan}).fillna("Missing")

    # One-Hot für Sex, dann konsequent numerisch
    X_dm = pd.get_dummies(X, columns=["Sex_norm"], drop_first=True)
    X_dm = X_dm.apply(pd.to_numeric, errors="coerce").replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float64)

    # Konstante & als NumPy
    X_dm = sm.add_constant(X_dm, has_constant="add")
    X_mat = np.asarray(X_dm.values, dtype=np.float64)
    y_vec = np.asarray(y, dtype=np.float64)

    model = sm.GLM(y_vec, X_mat, family=sm.families.Binomial())
    # Cluster-Gruppen als Codes (PMID -> int)
    groups = df["PMID"].astype("category").cat.codes.to_numpy()
    # <<< NEU: robust schon beim Fit >>>
    res = model.fit(
    cov_type="cluster",
    cov_kwds={"groups": groups, "use_correction": True}
    )

    # Cluster-Gruppen als Codes
    groups = df["PMID"].astype("category").cat.codes.to_numpy()
  
    params = res.params
    se     = res.bse
    z      = res.tvalues
    p      = res.pvalues
    ci_lo  = params - 1.96 * se
    ci_hi  = params + 1.96 * se
    

    terms = list(X_dm.columns)
    or_tab = pd.DataFrame({
        "Term": terms,
        "Coef": params,
        "OR": np.exp(params),
        "CI_low": np.exp(ci_lo),
        "CI_high": np.exp(ci_hi),
        "z": z,
        "p": p,
    })

    # Intercept zuerst
    intercept = or_tab[or_tab["Term"]=="const"]
    body = or_tab[or_tab["Term"]!="const"].sort_values("Term")
    out = pd.concat([intercept, body], axis=0) if not intercept.empty else body

    os.makedirs(save_dir, exist_ok=True)
    path = os.path.join(save_dir, "S4_glm_cluster_or.csv")
    out.to_csv(path, index=False)
    print("GLM (cluster-robust) OR-Tabelle gespeichert:", path)
    return path

def _run_groupkfold_cv(df, save_dir, n_splits=5):
    """GroupKFold (Gruppen=PMID) – ROC/PR/Kalibration + Metriken."""
    from sklearn.model_selection import GroupKFold
    from sklearn.impute import SimpleImputer
    from sklearn.preprocessing import OneHotEncoder, StandardScaler
    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, roc_curve, precision_recall_curve
    from sklearn.calibration import calibration_curve

    y = df["event_idx"].values
    groups = df["PMID"].values
    X = df[["duration_hours","is_reop","Sex_norm"]]

    num_feats = ["duration_hours","is_reop"]
    cat_feats = ["Sex_norm"]

    pre = ColumnTransformer([
        ("num", Pipeline([("imp", SimpleImputer(strategy="median")),("sc", StandardScaler())]), num_feats),
        ("cat", Pipeline([("imp", SimpleImputer(strategy="most_frequent")),("oh", OneHotEncoder(handle_unknown="ignore"))]), cat_feats)
    ])

    clf = Pipeline([("pre", pre), ("lr", LogisticRegression(max_iter=200, class_weight="balanced", solver="lbfgs"))])

    gkf = GroupKFold(n_splits=min(n_splits, len(np.unique(groups))))
    proba = np.empty_like(y, dtype=float)

    for i,(tr,te) in enumerate(gkf.split(X, y, groups),1):
        clf.fit(X.iloc[tr], y[tr])
        proba[te] = clf.predict_proba(X.iloc[te])[:,1]
        print(f"Fold {i} train={len(tr)} test={len(te)}")

    # Kennzahlen
    from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss
    roc_auc = roc_auc_score(y, proba)
    pr_auc = average_precision_score(y, proba)
    brier = brier_score_loss(y, proba)

    # Plots
    fpr,tpr,_ = roc_curve(y, proba)
    plt.figure(figsize=(6,5)); plt.plot(fpr,tpr); plt.plot([0,1],[0,1],'--'); plt.xlabel("FPR"); plt.ylabel("TPR"); plt.title("S4 ROC (GroupKFold)"); plt.tight_layout()
    roc_path = os.path.join(save_dir,"S4_ROC.png"); plt.savefig(roc_path,dpi=150); plt.close()

    prec,rec,_ = precision_recall_curve(y, proba)
    plt.figure(figsize=(6,5)); plt.plot(rec,prec); plt.xlabel("Recall"); plt.ylabel("Precision"); plt.title("S4 Precision-Recall (GroupKFold)"); plt.tight_layout()
    pr_path = os.path.join(save_dir,"S4_PR.png"); plt.savefig(pr_path,dpi=150); plt.close()

    prob_true, prob_pred = calibration_curve(y, proba, n_bins=10, strategy="quantile")
    plt.figure(figsize=(6,5)); plt.plot(prob_pred, prob_true); plt.plot([0,1],[0,1],'--'); plt.xlabel("Vorhergesagte Wahrscheinlichkeit"); plt.ylabel("Beobachteter Anteil"); plt.title("S4 Kalibration (GroupKFold)"); plt.tight_layout()
    cal_path = os.path.join(save_dir,"S4_Calibration.png"); plt.savefig(cal_path,dpi=150); plt.close()

    metrics = {"roc_auc": float(roc_auc), "pr_auc": float(pr_auc), "brier": float(brier),
               "roc_path": roc_path, "pr_path": pr_path, "calibration_path": cal_path}
    pd.DataFrame([metrics]).to_csv(os.path.join(save_dir,"S4_cv_metrics.csv"), index=False)
    print(f"CV-Metriken: ROC_AUC={roc_auc:.3f}, PR_AUC={pr_auc:.3f}, Brier={brier:.3f}")
    return metrics

def main_s4():
    """S4 ausführen und Ergebnisse in adata.uns ablegen (nur einfache Typen)."""
    from anndata import read_h5ad
    # CFG kommt aus deinem bestehenden Skript (S1/S2)
    h5 = CFG.OUT_H5AD if hasattr(CFG, "OUT_H5AD") else CFG.PATH_H5AD
    adata = read_h5ad(h5)
   # 1) Daten vorbereiten und Modelle rechnen
    df = _prepare_model_df(adata)
    or_csv = _fit_glm_clustered(df, CFG.SAVE_DIR)
    metrics = _run_groupkfold_cv(df, CFG.SAVE_DIR, n_splits=5)

    # 2) Visualisierungen HIER aufrufen (brauchen 'or_csv' und 'df')
    try:
        forest_png = s4_plot_forest(or_csv, CFG.SAVE_DIR)
    except Exception as e:
        print("Forest-Plot Fehler:", e); forest_png = ""
    try:
        margins_png = s4_plot_margins_duration(df, CFG.SAVE_DIR)
    except Exception as e:
        print("Margins-Plot Fehler:", e); margins_png = ""

    # 3) nur einfache Typen in uns (H5AD-sicher)
    try:
        k_terms = int(len(pd.read_csv(or_csv)))
    except Exception:
        k_terms = 0
    adata.uns["S4_glm"] = {
        "or_table_path": or_csv,
        "forest_path": forest_png,
        "margins_duration_path": margins_png,
        "n": int(df.shape[0]),
        "k": k_terms
    }
    adata.uns["S4_cv"] = metrics

    # 4) speichern
    adata.write_h5ad(h5)
    print(f"AnnData (S4) gespeichert: {h5}")
        # Visualisierungen
    s4_plot_forest(or_csv, CFG.SAVE_DIR)
    s4_plot_margins_duration(df, CFG.SAVE_DIR)
    s4_plot_margins_duration_by_reop(df, CFG.SAVE_DIR)   # ← neu

    # ===== S4: Visualisierungen =====
import matplotlib.pyplot as plt

def s4_plot_forest(or_csv_path: str, save_dir: str) -> str:
    """Forest-Plot für ORs (exkl. Intercept)."""
    import pandas as pd, numpy as np, os
    df = pd.read_csv(or_csv_path)
    df = df[df["Term"] != "const"].copy()
    if df.empty:
        return ""
    # Sortierung: erst binär/kategorial, dann kontinuierlich (oder einfach alphabetisch)
    df = df.sort_values("Term")
    terms = df["Term"].tolist()
    OR = df["OR"].values
    lo = df["CI_low"].values
    hi = df["CI_high"].values

    y = np.arange(len(terms))[::-1]
    plt.figure(figsize=(7, 0.6*len(terms) + 1))
    plt.hlines(y, lo, hi)
    plt.plot(OR, y, "o")
    plt.vlines(1.0, -1, len(terms), linestyles="--")
    plt.yticks(y, terms)
    plt.xlabel("Odds Ratio (log-Skala)")
    plt.xscale("log")
    plt.title("S4 – Odds Ratios (95%-KI)")
    plt.tight_layout()
    out = os.path.join(save_dir, "S4_forest_or.png")
    plt.savefig(out, dpi=150); plt.close()
    print("Forest-Plot gespeichert:", out)
    return out


#Plot

def s4_plot_margins_duration(df: pd.DataFrame, save_dir: str) -> str:
    """Marginaleffekt: p(AKI) über OP-Dauer (Baseline: Erst-OP, Referenz-Geschlecht)."""
    import statsmodels.api as sm, numpy as np, pandas as pd, os, math

    # === Daten & Dummy-Matrix wie im GLM ===
    X = df[["duration_hours", "is_reop", "Sex_norm"]].copy()
    X["duration_hours"] = pd.to_numeric(X["duration_hours"], errors="coerce")
    X["duration_hours"] = X["duration_hours"].fillna(X["duration_hours"].median())
    X["is_reop"] = pd.to_numeric(X["is_reop"], errors="coerce").fillna(0).astype(int)
    X["Sex_norm"] = X["Sex_norm"].astype(str).replace({"nan": np.nan, "None": np.nan}).fillna("Missing")

    X_dm = pd.get_dummies(X, columns=["Sex_norm"], drop_first=True)
    X_dm = X_dm.apply(pd.to_numeric, errors="coerce").replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float64)
    X_dm_const = sm.add_constant(X_dm, has_constant="add")

    y = pd.to_numeric(df["event_idx"], errors="coerce").astype(int).values
    groups = df["PMID"].astype("category").cat.codes.to_numpy()

    # === GLM mit cluster-robusten SE direkt beim Fit ===
    model = sm.GLM(y, X_dm_const.values, family=sm.families.Binomial())
    res = model.fit(cov_type="cluster", cov_kwds={"groups": groups, "use_correction": True})
    params = res.params
    cov = res.cov_params()

    # === Grid für Dauer (5.–95. Perzentil), Baseline: Erst-OP (is_reop=0), Referenz-Geschlecht ===
    q = X["duration_hours"].quantile([0.05, 0.95]).values
    grid = np.linspace(q[0], q[1], 60)

    # Designmatrix für Grid mit exakt gleichen Spalten wie X_dm_const
    cols = list(X_dm_const.columns)
    Xg = pd.DataFrame(0.0, index=np.arange(len(grid)), columns=cols, dtype=np.float64)
    Xg["const"] = 1.0
    if "duration_hours" in Xg.columns:
        Xg["duration_hours"] = grid
    if "is_reop" in Xg.columns:
        Xg["is_reop"] = 0.0  # Baseline: Erst-OP
    # alle Sex-Dummies bleiben 0 → Referenzkategorie

    # Vorhersage auf Link-Skala + Delta-Methode für 95%-KI, dann inverse Logit
    Xg_mat = Xg.values
    eta = Xg_mat @ params
    var = np.einsum("ij,jk,ik->i", Xg_mat, cov, Xg_mat)
    se = np.sqrt(np.clip(var, 0, np.inf))

    lo_eta = eta - 1.96*se
    hi_eta = eta + 1.96*se
    invlogit = lambda z: 1.0/(1.0+np.exp(-z))
    p = invlogit(eta)
    p_lo = invlogit(lo_eta)
    p_hi = invlogit(hi_eta)

    # Plotten
    plt.figure(figsize=(7,5))
    plt.plot(grid, p, label="geschätzt")
    plt.fill_between(grid, p_lo, p_hi, alpha=0.2, label="95%-KI")
    plt.xlabel("OP-Dauer (Stunden)")
    plt.ylabel("Prädizierte AKI-Wahrscheinlichkeit (0–7 Tage)")
    plt.title("S4 – Marginaleffekt der OP-Dauer (Baseline: Erst-OP, Referenz-Geschlecht)")
    plt.legend()
    plt.tight_layout()
    out = os.path.join(save_dir, "S4_margins_duration.png")
    plt.savefig(out, dpi=150); plt.close()
    print("Marginaleffekte gespeichert:", out)
    return out
# Neu: Marginaleffekte der OP-Dauer nach Re-OP-Typen
def s4_plot_margins_duration_by_reop(df: pd.DataFrame, save_dir: str) -> str:
    """
    Zwei Kurven: p(AKI) ~ duration_hours getrennt nach is_reop=0 vs. 1.
    Modell: lineares Logit wie im GLM (cluster-robust).
    """
    import statsmodels.api as sm

    # === Daten aufbereiten wie im GLM ===
    X = df[["duration_hours", "is_reop", "Sex_norm"]].copy()
    X["duration_hours"] = pd.to_numeric(X["duration_hours"], errors="coerce")
    X["duration_hours"] = X["duration_hours"].fillna(X["duration_hours"].median())
    X["is_reop"] = pd.to_numeric(X["is_reop"], errors="coerce").fillna(0).astype(int)
    X["Sex_norm"] = X["Sex_norm"].astype(str).replace({"nan": np.nan, "None": np.nan}).fillna("Missing")

    X_dm = pd.get_dummies(X, columns=["Sex_norm"], drop_first=True)
    X_dm = X_dm.apply(pd.to_numeric, errors="coerce").replace([np.inf,-np.inf], np.nan).fillna(0.0).astype(np.float64)
    X_dm_const = sm.add_constant(X_dm, has_constant="add")

    y = pd.to_numeric(df["event_idx"], errors="coerce").astype(int).values
    groups = df["PMID"].astype("category").cat.codes.to_numpy()

    model = sm.GLM(y, X_dm_const.values, family=sm.families.Binomial())
    res = model.fit(cov_type="cluster", cov_kwds={"groups": groups, "use_correction": True})
    params = res.params; cov = res.cov_params()

    # Grid 5.–95. Perzentil
    q = X["duration_hours"].quantile([0.05, 0.95]).values
    grid = np.linspace(q[0], q[1], 80)

    # Spaltennamen merken
    cols = list(X_dm_const.columns)

    def predict_curve(is_reop_val: int):
        Xg = pd.DataFrame(0.0, index=np.arange(len(grid)), columns=cols, dtype=np.float64)
        Xg["const"] = 1.0
        if "duration_hours" in Xg.columns: Xg["duration_hours"] = grid
        if "is_reop" in Xg.columns: Xg["is_reop"] = float(is_reop_val)
        # Sex-Dummies = 0 -> Referenz
        Xg_mat = Xg.values
        eta = Xg_mat @ params
        var = np.einsum("ij,jk,ik->i", Xg_mat, cov, Xg_mat)
        se = np.sqrt(np.clip(var, 0, np.inf))
        invlogit = lambda z: 1/(1+np.exp(-z))
        return invlogit(eta), invlogit(eta-1.96*se), invlogit(eta+1.96*se)

    p0, lo0, hi0 = predict_curve(0)  # Erst-OP
    p1, lo1, hi1 = predict_curve(1)  # Re-OP

    # Plotten
    plt.figure(figsize=(7,5))
    plt.plot(grid, p0, label="Erst-OP"); plt.fill_between(grid, lo0, hi0, alpha=0.15)
    plt.plot(grid, p1, label="Re-OP");  plt.fill_between(grid, lo1, hi1, alpha=0.15)
    plt.xlabel("OP-Dauer (Stunden)"); plt.ylabel("Prädizierte AKI-Wahrscheinlichkeit (0–7 Tage)")
    plt.title("S4 – Marginaleffekte der OP-Dauer nach OP-Typ")
    plt.legend(); plt.tight_layout()
    out = os.path.join(save_dir, "S4_margins_duration_by_reop.png")
    plt.savefig(out, dpi=150); plt.close()
    print("Margins (Erst-OP vs. Re-OP) gespeichert:", out)
    return out






if __name__ == "__main__":
    main()     # S1
    main_s2()  # S2
    main_s4()  # S4  
    
    
    

# %%
